# AnySkin Crawler — Architecture Guide

Monorepo for crawling, processing, and aggregating beauty/skincare product data from German retailers (DM, Rossmann, Mueller) and video content from social platforms.

## Repository Layout

```
crawler/
├── server/             # Payload CMS + Next.js (admin UI, REST API, PostgreSQL)
│   ├── src/
│   │   ├── payload.config.ts
│   │   ├── collections/        # 21 Payload collection configs
│   │   ├── actions/            # Server-side actions (seed, crawl, download)
│   │   └── app/(payload)/api/  # REST + GraphQL routes (auto-generated by Payload)
│   └── AGENTS.md               # Payload CMS development rules & patterns
├── worker/             # Standalone Node.js process (claims jobs, does the work)
│   ├── src/
│   │   ├── worker.ts           # Main loop (~1000 lines, 7 job handlers)
│   │   └── lib/                # All worker logic
│   └── AGENTS.md               # Worker architecture & internals
├── eslint.config.mjs
├── README.md
└── TODO.md
```

## Server

- **Payload CMS 3.x** on Next.js 15, React 19
- **Database**: PostgreSQL via `@payloadcms/db-postgres`
- **Auth**: `users` collection (admin UI) + `workers` collection (API key auth for workers)
- **No custom endpoints** — workers use Payload's standard REST API (`/api/<collection>`)
- Jobs are created in the admin UI; workers poll and process them autonomously
- See `server/AGENTS.md` for Payload CMS development patterns

### Running

```bash
cd server && pnpm dev   # http://localhost:3000
```

### Key env vars

```
DATABASE_URL=postgres://...
PAYLOAD_SECRET=...
STORAGE_ADAPTER=local|s3              # default: local
S3_BUCKET=anyskin-media               # only when STORAGE_ADAPTER=s3
S3_ACCESS_KEY_ID=...
S3_SECRET_ACCESS_KEY=...
S3_REGION=us-east-1
S3_ENDPOINT=http://localhost:9000     # for non-AWS (MinIO, etc.)
```

## Worker

Standalone Node.js process that polls for jobs and processes them autonomously. All business logic (claiming, matching, classification, persistence) runs in the worker, communicating with the server only via Payload's REST API.

See `worker/AGENTS.md` for full worker architecture, job types, data flow, and internals.

### Running

```bash
cd worker && pnpm worker   # uses tsx, no build step needed
```

### Key env vars

```
WORKER_SERVER_URL=http://localhost:3000
WORKER_API_KEY=<api-key-from-workers-collection>
WORKER_POLL_INTERVAL=10          # seconds between idle polls
LOG_LEVEL=debug|info|warn|error  # default: info
OPENAI_API_KEY=sk-...            # for LLM tasks
DEEPGRAM_API_KEY=...             # for speech-to-text transcription
```

## Database Schema (21 Collections)

### Core Data

| Collection | Purpose |
|------------|---------|
| `products` | Unified product records (name, GTIN, brand, productType, image, ingredients, attributes, claims, warnings, skinApplicability, phMin/phMax, usageInstructions, usageSchedule) |
| `source-products` | Raw crawled data per retailer (status: uncrawled/crawled/failed, source: dm/mueller/rossmann, sourceUrl unique, categoryBreadcrumb text) |
| `brands` | Brand names |
| `product-types` | Skincare types (cleanser, toner, moisturizer, etc.) with DE/EN names |
| `ingredients` | Ingredient database (name, CAS#, EC#, CosIng ID, functions, restrictions) |

### Video Data

| Collection | Purpose |
|------------|---------|
| `videos` | YouTube/social videos (channel ref, title, duration, processingStatus, transcript, transcriptWords) |
| `video-snippets` | Video segments (timestamps, matchingType: barcode/visual, screenshots, referencedProducts, preTranscript/transcript/postTranscript) |
| `video-mentions` | Product-specific quotes from video snippets (videoSnippet ref, product ref, quotes with sentiment scores) |
| `creators` | Social media creators |
| `channels` | Creator channels (platform: youtube/instagram/tiktok, image, canonicalUrl for dedup) |

### Job Collections

All follow status lifecycle: `pending` → `in_progress` → `completed|failed`

| Collection | Key fields |
|------------|------------|
| `product-crawls` | source, type (all/selected_urls/selected_gtins/from_discovery), scope, progress |
| `product-discoveries` | sourceUrls, progress, discovered/created/existing counts |
| `ingredients-discoveries` | sourceUrl, currentTerm/Page, termQueue |
| `video-discoveries` | channelUrl, created/existing counts |
| `video-processings` | type (all_unprocessed/single_video/selected_urls), transcription config (language, model, enabled), processed/errors/tokens (total + per-step) |
| `product-aggregations` | type (all/selected_gtins), language, imageSourcePriority, aggregated/errors/tokens |

### System

| Collection | Purpose |
|------------|---------|
| `users` | Admin users (Payload auth) |
| `workers` | Worker processes (API key auth, capabilities list, lastSeenAt) |
| `events` | Structured audit log (type, level, component, message, job polymorphic relation, labels) |
| `media` | File uploads |
| `crawl-results` | Join table: product-crawls → source-products (hidden) |
| `discovery-results` | Join table: product-discoveries → source-products (hidden) |

## End-to-End Data Flow

```
1. Admin creates job in Payload UI (e.g. product-crawl with source=dm)
2. Worker polls → claimWork() finds pending job → builds work unit
3. Worker runs handler (e.g. scrapes product pages via Playwright driver)
4. Worker calls submitWork() → persist functions create/update DB records
5. Worker loops back, claims next batch until job completes
6. Product aggregation merges source-products by GTIN → selects best image (by source priority) → downloads & uploads to media → matchBrand + matchIngredients + classifyProduct (LLM) → unified products
7. Video processing: download → scene detect → barcode/visual match → audio transcription (Deepgram) → LLM correction → transcript split → sentiment analysis (LLM) → video-mentions
```

## Keeping AGENTS.md Up to Date

Whenever you make changes to the codebase, **update the relevant AGENTS.md file(s)** to reflect those changes. This is mandatory — documentation must stay in sync with the code.

- **Root `AGENTS.md`** (this file): Update when changes affect the overall repository layout, database schema (adding/removing/renaming collections), end-to-end data flow, environment variables, or cross-cutting development notes.
- **`server/AGENTS.md`**: Update when changes affect Payload CMS collections, fields, hooks, access control, components, actions, endpoints, or server-side patterns.
- **`worker/AGENTS.md`**: Update when changes affect job handlers, the work protocol, source drivers, matching/classification functions, the REST client, logging, or worker-side patterns.

If a change spans both server and worker (e.g. adding a new job type with a new collection and a new handler), update all three files accordingly.

## Development Notes

- **TypeScript** throughout; worker uses `@/` path aliases via tsconfig
- **No build step** for worker in dev — runs via `tsx`
- **Server types**: Run `pnpm generate:types` after collection schema changes
- **Tests**: Server has Vitest (integration) + Playwright (e2e) in `server/tests/`
