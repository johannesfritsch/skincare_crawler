# AnySkin Crawler — Architecture Guide

Monorepo for crawling, processing, and aggregating beauty/skincare product data from retailers and video content from social platforms.

**Currently supported stores**: DM, Rossmann, Mueller (3 German drugstores). The architecture is designed to scale to **10-15 stores** total — new stores are added by implementing a new source driver in `worker/src/lib/source-discovery/drivers/`. When adding a store, keep this in mind: all store-specific logic must live in the driver; shared code (persist, aggregation, matching) must remain store-agnostic. Avoid hardcoding store slugs in shared paths — use the driver registry pattern (`getSourceDriver`/`getSourceDriverBySlug`) and the `source` field on source-products. The `source` select field on collections and the `SourceSlug` type will need new options for each store.

## Repository Layout

```
crawler/
├── server/             # Payload CMS + Next.js (admin UI, REST API, PostgreSQL)
│   ├── src/
│   │   ├── payload.config.ts
│   │   ├── collections/        # 23 Payload collection configs
│   │   ├── actions/            # Server-side actions (seed, crawl, download)
│   │   └── app/(payload)/api/  # REST + GraphQL routes (auto-generated by Payload)
│   └── AGENTS.md               # Payload CMS development rules & patterns
├── worker/             # Standalone Node.js process (claims jobs, does the work)
│   ├── src/
│   │   ├── worker.ts           # Main loop (~1000 lines, 7 job handlers)
│   │   └── lib/                # All worker logic
│   └── AGENTS.md               # Worker architecture & internals
├── eslint.config.mjs
├── README.md
└── TODO.md
```

## Server

- **Payload CMS 3.x** on Next.js 15, React 19
- **Database**: PostgreSQL via `@payloadcms/db-postgres`
- **Auth**: `users` collection (admin UI) + `workers` collection (API key auth for workers)
- **No custom endpoints** — workers use Payload's standard REST API (`/api/<collection>`)
- Jobs are created in the admin UI; workers poll and process them autonomously
- See `server/AGENTS.md` for Payload CMS development patterns

### Running

```bash
cd server && pnpm dev   # http://localhost:3000
```

### Key env vars

```
DATABASE_URL=postgres://...
PAYLOAD_SECRET=...
STORAGE_ADAPTER=local|s3              # default: local
S3_BUCKET=anyskin-media               # only when STORAGE_ADAPTER=s3
S3_ACCESS_KEY_ID=...
S3_SECRET_ACCESS_KEY=...
S3_REGION=us-east-1
S3_ENDPOINT=http://localhost:9000     # for non-AWS (MinIO, etc.)
```

## Worker

Standalone Node.js process that polls for jobs and processes them autonomously. All business logic (claiming, matching, classification, persistence) runs in the worker, communicating with the server only via Payload's REST API.

See `worker/AGENTS.md` for full worker architecture, job types, data flow, and internals.

### Running

```bash
cd worker && pnpm worker   # uses tsx, no build step needed
```

### Key env vars

```
WORKER_SERVER_URL=http://localhost:3000
WORKER_API_KEY=<api-key-from-workers-collection>
WORKER_POLL_INTERVAL=10          # seconds between idle polls
WORKER_JOB_TIMEOUT_MINUTES=30   # minutes before abandoned job can be reclaimed
LOG_LEVEL=debug|info|warn|error  # default: info
OPENAI_API_KEY=sk-...            # for LLM tasks
DEEPGRAM_API_KEY=...             # for speech-to-text transcription
```

## Database Schema (27 Collections)

### Core Data

| Collection | Purpose |
|------------|---------|
| `products` | Unified product records (name, brand, productType, image, ingredients, attributes, claims, warnings, skinApplicability, phMin/phMax, usageInstructions, usageSchedule, scoreHistory with change: drop/stable/increase). No longer has `gtin` — GTINs live on `product-variants`. Has a `variants` join to product-variants collection. |
| `product-variants` | Per-variant records for unified products. Each has: `product` (required relationship → products, indexed), `gtin` (unique, indexed), `label`, `image` (upload → media), `sourceVariants` (hasMany relationship → source-variants), `isDefault` (checkbox). Aggregation pipeline creates these when creating/updating products. Frontend routes via `/products/[gtin]` look up product-variants first. |
| `source-products` | Raw crawled data per retailer (status: uncrawled/crawled/failed, source: dm/mueller/rossmann, name, brand, images, categoryBreadcrumb text, ingredientsText raw textarea, priceHistory with change: drop/stable/increase, rating, ratingNum, description). No longer has gtin or sourceUrl — those moved to source-variants. Has a `sourceVariants` join to source-variants collection. |
| `source-variants` | Per-variant records for retailer products. Each has: `sourceProduct` (required relationship → source-products, indexed), `sourceUrl` (unique, indexed — the dedup key), `gtin` (indexed), `variantLabel`, `variantDimension`, `isDefault` (checkbox), `crawledAt` (date, tracks when this specific variant was last crawled). Every source-product has at least one default source-variant. Mueller variant options (via `?itemId=`) each get their own source-variant. |
| `brands` | Brand names |
| `product-types` | Skincare types (cleanser, toner, moisturizer, etc.) with DE/EN names |
| `ingredients` | Ingredient database (name, image, shortDescription, longDescription, description, CAS#, EC#, CosIng ID, functions, restrictions) |

### Video Data

| Collection | Purpose |
|------------|---------|
| `videos` | YouTube/social videos (channel ref, title, duration, processingStatus, transcript, transcriptWords) |
| `video-snippets` | Video segments (timestamps, matchingType: barcode/visual, screenshots, referencedProducts, preTranscript/transcript/postTranscript) |
| `video-mentions` | Product-specific quotes from video snippets (videoSnippet ref, product ref, quotes with sentiment scores) |
| `creators` | Social media creators |
| `channels` | Creator channels (platform: youtube/instagram/tiktok, image, canonicalUrl for dedup) |

### Job Collections

All follow status lifecycle: `pending` → `in_progress` → `completed|failed`

All job collections have `claimedBy` (relationship to workers) and `claimedAt` (date) fields for distributed locking. A `beforeChange` hook (`enforceJobClaim`) prevents two workers from claiming the same job. Jobs cycle through claim states: **pending** → worker claims (sets `claimedBy`/`claimedAt`) → processes batch → **releases claim** (`claimedBy`/`claimedAt` set to null) → any worker claims next batch → ... → **completed**. Stale claims (older than `WORKER_JOB_TIMEOUT_MINUTES`, default 30m) are automatically released and can be reclaimed by another worker. Workers refresh `claimedAt` via heartbeat to keep claims alive during long batches. Workers are fully stateless; all progress lives on the server.

| Collection | Key fields |
|------------|------------|
| `product-crawls` | source, type (all/selected_urls/selected_gtins/from_discovery), scope, crawlVariants (default true), progress |
| `product-discoveries` | sourceUrls, progress, discovered/created/existing counts |
| `product-searches` | query, sources (dm/mueller/rossmann), maxResults, discovered/created/existing counts |
| `ingredients-discoveries` | sourceUrl, currentTerm/Page, termQueue |
| `video-discoveries` | channelUrl, itemsPerTick (videos per batch, default 50), maxVideos, progress (currentOffset), created/existing/discovered counts |
| `video-processings` | type (all_unprocessed/single_video/selected_urls), transcription config (language, model, enabled), processed/errors/tokens (total + per-step) |
| `product-aggregations` | type (all/selected_gtins), scope (full/partial), language, imageSourcePriority, aggregated/errors/tokens |
| `ingredient-crawls` | type (all_uncrawled/selected), crawled/errors/tokens |

### System

| Collection | Purpose |
|------------|---------|
| `users` | Admin users (Payload auth) |
| `workers` | Worker processes (API key auth, capabilities list, lastSeenAt) |
| `events` | Structured audit log (type, level, component, message, job polymorphic relation, labels) |
| `media` | File uploads |
| `crawl-results` | Join table: product-crawls → source-products (hidden) |
| `discovery-results` | Join table: product-discoveries → source-products (hidden) |
| `search-results` | Join table: product-searches → source-products (hidden) |

## End-to-End Data Flow

```
1. Admin creates job in Payload UI (e.g. product-crawl with source=dm)
2. Worker polls → claimWork() finds pending job → builds work unit
3. Worker runs handler (e.g. scrapes product pages via Playwright driver)
4. Worker calls submitWork() → persist functions create/update DB records
5. Worker loops back, claims next batch until job completes
6. Product aggregation resolves GTINs via source-variants → groups source-products by GTIN → selects best image (by source priority) → downloads & uploads to media → matchBrand + parseIngredients (raw text → names via LLM) + matchIngredients + classifyProduct (LLM) → unified products + product-variants (with GTIN + source-variant links)
7. Video processing: download → scene detect → barcode/visual match → audio transcription (Deepgram) → LLM correction → transcript split → sentiment analysis (LLM) → video-mentions
```

## Keeping AGENTS.md Up to Date

Whenever you make changes to the codebase, **update the relevant AGENTS.md file(s)** to reflect those changes. This is mandatory — documentation must stay in sync with the code.

- **Root `AGENTS.md`** (this file): Update when changes affect the overall repository layout, database schema (adding/removing/renaming collections), end-to-end data flow, environment variables, or cross-cutting development notes.
- **`server/AGENTS.md`**: Update when changes affect Payload CMS collections, fields, hooks, access control, components, actions, endpoints, or server-side patterns.
- **`worker/AGENTS.md`**: Update when changes affect job handlers, the work protocol, source drivers, matching/classification functions, the REST client, logging, or worker-side patterns.

If a change spans both server and worker (e.g. adding a new job type with a new collection and a new handler), update all three files accordingly.

## Adding a New Store Driver

The system is designed to scale to 10-15 store drivers. Store-specific logic lives exclusively in driver files; shared code (persist, aggregation, matching, frontend display) is store-agnostic and derives configuration from centralized registries.

### Architecture: Where store slugs are defined

Store slugs are centralized in **two registry files** (one per process):

| Registry | Process | Exports | Consumers |
|----------|---------|---------|-----------|
| `worker/src/lib/source-discovery/driver.ts` | Worker | `ALL_SOURCE_SLUGS`, `DEFAULT_IMAGE_SOURCE_PRIORITY`, `getAllSourceDrivers()` | claim.ts, submit.ts, worker.ts, aggregate-product.ts, source-product-queries.ts |
| `server/src/collections/shared/store-fields.ts` | Server | `SOURCE_OPTIONS`, `SOURCE_OPTIONS_WITH_ALL`, `ALL_SOURCE_SLUGS`, `DEFAULT_IMAGE_SOURCE_PRIORITY`, `STORE_LABELS` | Collection configs (SourceProducts, ProductCrawls, ProductSearches, SearchResults, ProductAggregations), score-utils.tsx |

The worker registry derives its lists from the `drivers[]` array (each driver declares its own `slug`, `label`, `hosts`). Source filters and URL matchers in `source-product-queries.ts` are also derived from the driver registry automatically.

The server registry is a simple static list (server and worker are separate processes — no cross-process imports). Frontend display functions (`storeLabel()` in `score-utils.tsx`, `StoreLogo` in `store-logos.tsx`) use map lookups with graceful fallbacks for unknown slugs.

### Step-by-step: Adding a new store

1. **Create the driver** — `worker/src/lib/source-discovery/drivers/<slug>.ts`
   - Implement the `SourceDriver` interface: `slug`, `label`, `hosts`, `logoSvg`, `matches()`, `discoverProducts()`, `searchProducts()`, `scrapeProduct()`
   - Use existing drivers as reference (DM for API-based, Rossmann/Mueller for Playwright-based)

2. **Register the driver** — `worker/src/lib/source-discovery/driver.ts`
   - Import the driver and add it to the `drivers[]` array
   - `ALL_SOURCE_SLUGS` and `DEFAULT_IMAGE_SOURCE_PRIORITY` update automatically

3. **Update the `SourceSlug` type** — `worker/src/lib/source-discovery/types.ts`
   - Add the new slug to the `SourceSlug` union type (e.g. `'dm' | 'mueller' | 'rossmann' | 'newstore'`)

4. **Update server store registry** — `server/src/collections/shared/store-fields.ts`
   - Add `{ label: 'Store Name', value: 'newslug' }` to `SOURCE_OPTIONS`
   - All collection configs that use `SOURCE_OPTIONS` / `SOURCE_OPTIONS_WITH_ALL` / `ALL_SOURCE_SLUGS` / `DEFAULT_IMAGE_SOURCE_PRIORITY` update automatically

5. **Add the store logo** — `server/src/components/store-logos.tsx`
   - Create a new logo component function (inline SVG)
   - Add it to `LOGO_MAP` (the `StoreLogo` component dispatches via map lookup; unknown slugs fall back to text)

6. **Run `pnpm generate:types`** in the server to regenerate `payload-types.ts`

7. **Create a database migration** — `ALTER TYPE ... ADD VALUE 'newslug'` for each affected Postgres enum:
   - `enum_source_products_source`
   - `enum_product_crawls_source`
   - `enum_product_searches_sources`
   - `enum_search_results_source`

### What you do NOT need to touch

- `claim.ts`, `submit.ts`, `worker.ts`, `aggregate-product.ts` — all use `ALL_SOURCE_SLUGS` / `DEFAULT_IMAGE_SOURCE_PRIORITY` from the driver registry
- `source-product-queries.ts` — `SOURCE_FILTERS` and `URL_MATCHERS` are derived from the driver registry's `hosts` field
- `score-utils.tsx` — `storeLabel()` uses `STORE_LABELS` map with fallback to raw slug
- `trait-chip.tsx` — imports `storeLabel()` from `score-utils.tsx`
- Persist functions (`persist.ts`) — fully store-agnostic
- Aggregation logic — fully store-agnostic

## Development Notes

- **TypeScript** throughout; worker uses `@/` path aliases via tsconfig
- **No build step** for worker in dev — runs via `tsx`
- **Server types**: Run `pnpm generate:types` after collection schema changes
- **Tests**: Server has Vitest (integration) + Playwright (e2e) in `server/tests/`
